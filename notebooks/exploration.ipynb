{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c269b7",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Graduate Admission Predictor\n",
    "\n",
    "This notebook explores how academic profile features like GRE, TOEFL, and CGPA can be used to predict a student's chance of admission to graduate school using machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eb76b",
   "metadata": {},
   "source": [
    "## ðŸ“Š Dataset Overview\n",
    "\n",
    "We use the Kaggle Graduate Admissions dataset, which includes scores, ratings, and research experience. We'll load and clean it, then perform EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b73c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Load dataset\n",
    "df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"mohansacharya/graduate-admissions\",\n",
    "    \"Admission_Predict_Ver1.1.csv\"\n",
    ")\n",
    "df = df.rename(columns={'Chance of Admit ': 'Chance of Admit'})\n",
    "df.drop('Serial No.', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(9, 7))\n",
    "    sns.heatmap(corr, mask=mask, square=True, annot=True, fmt='0.2f', linewidths=.8, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"GRE Score\", y=\"TOEFL Score\", data=df)\n",
    "plt.title(\"GRE Score vs TOEFL Score\")\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x=\"GRE Score\", y=\"CGPA\", data=df)\n",
    "plt.title(\"GRE Score vs CGPA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8566201",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'CGPA']:\n",
    "    sns.histplot(df[col], kde=False)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a5255",
   "metadata": {},
   "source": [
    "## ðŸ¤– Model Training and Evaluation\n",
    "\n",
    "We'll normalize features, split the data, train several models, and compare performance using RMSE, MAE, and RÂ²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ab4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "X = df.drop('Chance of Admit', axis=1)\n",
    "y = df['Chance of Admit']\n",
    "X_norm = normalize(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressors = [\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Decision Tree\", DecisionTreeRegressor()),\n",
    "    (\"Random Forest\", RandomForestRegressor()),\n",
    "    (\"Gradient Boosting\", GradientBoostingRegressor()),\n",
    "    (\"Ada Boosting\", AdaBoostRegressor()),\n",
    "    (\"Extra Trees\", ExtraTreesRegressor()),\n",
    "    (\"K-Neighbors\", KNeighborsRegressor()),\n",
    "    (\"Support Vector\", SVR())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in regressors:\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    results.append((name, rmse, mae, r2))\n",
    "    print(f\"{name}: RMSE={rmse:.4f}, MAE={mae:.4f}, RÂ²={r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a110ad5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Importance\n",
    "\n",
    "We analyze feature contributions using Extra Trees and Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e163de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesRegressor().fit(X_norm, y)\n",
    "importances = model.feature_importances_\n",
    "sns.barplot(x=importances, y=X.columns)\n",
    "plt.title(\"Feature Importances (Extra Trees)\")\n",
    "plt.show()\n",
    "\n",
    "lr = LinearRegression().fit(X_norm, y)\n",
    "coef_importance = np.abs(lr.coef_)\n",
    "sns.barplot(x=coef_importance, y=X.columns)\n",
    "plt.title(\"Feature Importances (Linear Regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36f548",
   "metadata": {},
   "source": [
    "## ðŸ§ª Try Your Own Input\n",
    "\n",
    "Test the model with a sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c03c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = ExtraTreesRegressor(n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=2, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "sample = np.array([[320, 110, 4, 4, 4, 9.2, 1]])\n",
    "sample = normalize(sample)\n",
    "final_model.predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863c202",
   "metadata": {},
   "source": [
    "## âœ… Conclusion\n",
    "\n",
    "Extra Trees Regressor outperformed other models. CGPA, GRE, and TOEFL were the most influential features. This notebook demonstrates EDA, model evaluation, feature importance, and practical prediction workflows in Python."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}